# Two-Layer MLP on MNIST (from Scratch)

This project implements a **two-layer Multi-Layer Perceptron (MLP)** trained on the **MNIST dataset**.  
The goal is to demonstrate how a simple neural network can be built **from scratch** (without deep learning frameworks like TensorFlow or PyTorch) to classify handwritten digits (0â€“9).  

---

## âœ¨ Features
- Implementation of a **two-layer fully connected neural network**  
- **Forward propagation**, **loss computation**, and **backpropagation** implemented manually  
- Uses **stochastic gradient descent (SGD)** for optimization  
- Trained on the **MNIST handwritten digit dataset**  
- Achieves reasonable accuracy with minimal code  

---

## ðŸ“‚ Project Structure
```
â”œâ”€â”€ data/    
|   |â”€â”€ plots/      # stores all plots created from raw data
|   |â”€â”€ dataset.py              
|   |â”€â”€ preprocess.py              
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ mlp.py           
â”‚   â”œâ”€â”€ helpers.py  
â”œâ”€â”€ results/        # stores all graphs and images from training, validation and test
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ batch.py  
â”‚   â”œâ”€â”€ loss.py  
â”‚   â”œâ”€â”€ training_loop.py
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ metric_plots.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ plot_images.py
â”œâ”€â”€ README.md              
â”œâ”€â”€ .gitignore              
â”œâ”€â”€ main.py         # entry point of MLP              
â””â”€â”€ requirements.txt       
```


---

## ðŸ§  Model Architecture
The network consists of:
1. **Input Layer**: 784 units (28x28 flattened images)  
2. **Hidden Layer**: 50 neurons with Sigmoid activation  
3. **Output Layer**: 10 neurons with Sigmoid activation  

**Loss Function**: Sum of Squares  
**Optimizer**: SGD (with learning rate tuning)  

---

## ðŸš€ Installation & Usage
### 1. Clone Repository
```bash
git clone https://github.com/prabhsuratsingh/mnist-mlp.git
cd mnist-mlp
```

### 2. Install Requirements
```bash
pip install -r requirements.txt
```

### 3. Run Training
```bash
python main.py
```
